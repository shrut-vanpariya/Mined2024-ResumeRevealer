{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdf2image\n",
    "# !pip install pytesseract\n",
    "# !pip install docx\n",
    "# !pip install python-docx\n",
    "# !pip install textract\n",
    "# !pip install python-textract\n",
    "# !pip install libmagic\n",
    "# !pip install python-magic\n",
    "# !pip install python-magic-bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "from docx import Document\n",
    "import docx\n",
    "\n",
    "import textract\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# import magic\n",
    "\n",
    "import os\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'./tools/Tesseract-OCR/tesseract.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. text from txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Srikanth\n",
      "+1(415)-787-2274\n",
      "Srikanthk0718@gmail.com \n",
      "Senior Data Engineer\n",
      "\n",
      "\n",
      "SUMMARY:\n",
      "•\tA Senior Data Engineer with over 9+ years’ experience in Information Technology in all phases of Software Development with Agile methodology which includes User Interaction, Business Analysis/Modeling, Design & Development, Integration, Planning and testing, migration and documentation in applications using ETL pipelines and distributed applications. \n",
      "•\tHas expertise in data engineering for over 4 years using Hadoop big data technologies Apache Spark, Scala, python, Kafka, Cassandra, Jenkins Pipelines, Kubernetes, Kibana, Rancher, GITHUB, Rancher, Kibana, Hadoop HDFS, Hive, IntelliJ, Cassandra, SQL server etc. \n",
      "•\tWorked with various transformations like Normalizer, expression, rank, filter, group, aggregator, lookups, joiner, sequence generator, sorter, SQLT, stored procedure, Update strategy, Source Qualifier, Transaction Control, Union, CDC etc., \n",
      "•\tDesigning and developing Spark jobs with data frames using different file formats like Text, Sequence, Xml, parquet, and Avro.\n",
      "•\tExperience working with NoSQL database technologies, including MongoDB, Cassandra, and HBase. \n",
      "•\tExcellent experience with Requests, NumPy, Matplotlib, SciPy, PySpark and Pandas python libraries during development lifecycle and experience in developing APIs for the application using Python. \n",
      "•\tConstructing and manipulating large datasets of structured, semi-structured, and unstructured data and supporting system application architecture using tools like SAS, SQL, Python, R. \n",
      "•\tProficient of AWS services like VPC, Glue Pipelines, Glue Crawler, Cloud front, EC2, ECS, EKS, Elastic bean stalk, Lambda, S3, Storage gateway, RDBS, Dynamo db, Redshift, Elastic Cache, DMS, SMS, Data Pipeline, IAM, WAF, Artifacts, API gateway, SNS, SQS, SES, Auto Scaling, Cloud Formation, Cloud Watch and Cloud Trail. \n",
      "•\tGood Knowledge on Big Data Tools such as Apache Kafka, Apache Spark, Airflow, Hive, Sqoop, Delta Lake with different file formats like Avro, Csv and Parquet. \n",
      "•\tExpertise building data lakes and data warehouses in AWS using a variety of cloud services i.e., S3, RDS, EC2) and Azure services (Azure Data Lake, Azure Blob Storage, Azure Synapse Analytics, ADF) and processing the data in Azure Databricks. \n",
      "•\tStrong experience with spark real time streaming data using Kafka. \n",
      "•\tImplemented large Lambda architectures using Azure Data platform capabilities like Azure Data Lake, Azure Data \n",
      "•\tFactory, HDInsight, Azure SQL Server, Azure ML and Power BI. \n",
      "•\tValidating data against files and performing technical data quality checks to certify source and target/business usage. \n",
      "•\tVery good in data modeling knowledge in Dimensional Data modeling, Star Schema, Snow-Flake Schema, FACT and Dimensions tables. \n",
      "•\tWorked on optimizing volumes and EC2 instances and created multiple VPC instances. Deployed applications on AWS using Elastic Beanstalk and Implemented and set up Route53 for AWS Web Instances. \n",
      "•\tCoordinating with Business Users, functional Design team and testing team during the different phases of project development and resolving the issues. \n",
      "•\tWorked with different database oracle, SQL Server, Teradata, Cassandra SQL Programming. \n",
      "•\tGood knowledge on Teradata and Snowflake databases.\n",
      "\n",
      "\n",
      "TECHNICAL SKILLS:\n",
      "\n",
      "\n",
      "Big Data Tools: Hadoop Ecosystem: Map Reduce, Spark 2.3, Airflow 1.10.8, Nifi 2, HBase 1.2, Hive 2.3, Pig 0.17 Sqoop 1.4, Kafka 1.0.1, Oozie 4.3, Hadoop 3.0\n",
      "BI Tools: SSIS, SSRS, SSAS.\n",
      "Data Modeling Tools: Erwin Data Modeler, ER Studio v17\n",
      "Programming Languages: SQL, PL/SQL, and UNIX.\n",
      "Methodologies: RAD, JAD, System Development Life Cycle (SDLC), Agile\n",
      "Cloud Platform: AWS, Azure, Google Cloud.\n",
      "Cloud Management: Amazon Web Services (AWS)- EC2, EMR, S3, Redshift, EMR, Lambda, Athena\n",
      "Databases: Oracle 12c/11g, Teradata R15/R14.\n",
      "OLAP Tools: Tableau, SSAS, Business Objects, and Crystal Reports 9\n",
      "ETL/Data warehouse Tools: Informatica 9.6/9.1, and Tableau.\n",
      "Operating System: Windows, Unix, Sun Solaris\n",
      "\n",
      "\n",
      "PROFESSIONAL EXPERIENCE:\n",
      "John Deere - Urbandale - Iowa\t\t\t\t        May 2020 to Present\n",
      "Senior Data Engineer\n",
      "•\tParticipate in requirement grooming meetings which involves understanding functional requirements from business perspective and providing estimates to convert those requirements into software solutions (Design and Develop & Deliver the Code to IT/UAT/PROD and validate and manage data Pipelines from multiple applications with fast-paced Agile Development methodology using Sprints with JIRA Management Tool) \n",
      "•\tCreation and deployment of Spark jobs in different environments and loading data to no SQL database Cassandra/Hive/HDFS. Secure the data by implementing encryption-based \n",
      "•\tImplemented AWS solutions using E2C, S3, RDS, EBS, Elastic Load Balancer, Glue Pipelines, Glue Crawler, Auto scaling groups, Optimized volumes, and EC2 instances and created monitors, alarms, and notifications for EC2 hosts using Cloud Watch. \n",
      "•\tExperience in developing multiple MapReduce programs in java for data extraction, transformation and aggregation from multiple file formats including XML, JSON, CSV, and other file formats. \n",
      "•\tDeveloping code using: Apache Spark and Scala, IntelliJ, NoSQL databases (Cassandra), Jenkins, Docker pipelines, GITHUB, Kubernetes, HDFS file System, Hive, Kafka for streaming Real time streaming data, Kibana for monitor logs. \n",
      "•\tAnalysis on existing data flows and create high level/low level technical design documents for business stakeholders that confirm technical design aligns with business requirements. \n",
      "•\tExtensively worked on Data Modeling involving Dimensional Data modeling, Star Schema/Snowflake schema, FACT & Dimensions tables, Physical & logical data modeling. \n",
      "•\tDeveloped data models and data migration strategies utilizing concepts of snowflake schema. \n",
      "•\tInvolving in testing the database using complex SQL scripts and handling the performance issues effectively. \n",
      "•\tHandled importing of data from various data sources, performed transformations using Hive, MapReduce, loaded data into HDFS and extracted data from MYSQL into HDFS vice-versa using Sqoop. \n",
      "•\tETL development using EMR/Hive/Spark, Lambda, Scala, DynamoDB Streams, Amazon Kinesis Firehose, Redshift and S3. \n",
      "•\tLoaded the data into Spark RDD, perform advanced procedures like text analytics and processing using in-memory data computation’s capabilities of Spark using Scala to generate the Output response. \n",
      "•\tUsed Spark API over Cloudera Hadoop Yarn to perform analytics on data in Hive. \n",
      "•\tDeveloped Scala scripts using both Data frames/SQL and RDD/MapReduce in Spark for Data Aggregation, queries and writing data back into the OLTP system through Sqoop.\n",
      "•\tWrote various data normalization jobs for new data ingested into Redshift. \n",
      "•\tUsed AWS S3 Buckets to store the file and injected the files into Snowflake tables using Snow Pipe and run deltas using Data pipelines. \n",
      "•\tHandled large datasets using Partitions, Spark in Memory capabilities, Broadcasts in Spark, Effective efficient Joins, Transformations, and others during the ingestion process itself. \n",
      "•\tWorked with AWS cloud platform and its features which include EC2, IAM, EBS CloudWatch and AWS S3 Deployed application using AWS EC2 standard deployment techniques and worked on AWS infrastructure and automation. \n",
      "•\tInstalled and configured Apache airflow for workflow management and created workflows in python. \n",
      "•\tDeveloped python code for different tasks, dependencies, SLA watcher and time sensor for each job for workflow management and automation using Airflow tool. \n",
      "•\tImplementing the strategy to migrate Netezza based analytical systems to Snowflake on AWS. \n",
      "Environment: Apache spark 2.4.5, Scala2.1.1, Cassandra, HDFS, Hive, GitHub, Jenkins, Kafka, Informatica PowerCenter 10.x, SQL Server 2008, Salesforce Cloud, Visio, TOAD, Putty, Autosys Scheduler, UNIX, AWS, snowflake, CSV, WinSCP, python\n",
      "\n",
      "American Airlines, Fort Worth, Texas\t\t\t\t               Feb 2017 to Apr 2020\n",
      "Senior Data Engineer\n",
      "•\tDesigned and implemented scalable infrastructure and platform for large amounts of data ingestion, aggregation, integration, and analytics in Hadoop, including Spark, Hive, HBase. \n",
      "•\tCreating job workflows using Oozie scheduler. \n",
      "•\tDeveloping an architecture to move the project from Abinitio to Pyspark and Scala spark. \n",
      "•\tImplemented enterprise grade platform (Mark logic) for ETL from mainframe to NoSQL (Cassandra). \n",
      "•\tExtract Transform and Load data from Sources Systems to Azure Data Storage services using a combination of Azure Data Factory, T-SQL, Spark SQL, and U-SQL Azure Data Lake Analytics. Data Ingestion to one or more Azure Services - (Azure Data Lake, Azure Storage, Azure SQL, Azure DW) and processing the data in In Azure Databricks. \n",
      "•\tUse Python, Scala programming daily to perform transformations for applying business logic. \n",
      "•\tInvolved in designing, developing, testing, and documenting an application to combine personal loan, credit card and mortgage from different countries and load data to Sybase database from hive database for Reporting insights.\n",
      "•\tBuilding distributed data scalable using Hadoop. \n",
      "•\tUsing Sqoop to load data from HDFS, Hive, MySQL, and many other sources on daily bases. \n",
      "•\tCreating MapReduce programs to enable data for transformation, extraction, and aggregation of multiple formats like Avro, Parquet, XML, JSON, CSV, and other compressed file formats. \n",
      "•\tWriting Hive Queries in Spark-SQL for analysis and processing the data.\n",
      "•\tExported the analyzed data into relational databases using Sqoop for visualization and to generate reports for the BI team. \n",
      "•\tConverting data load pipeline algorithms written in python and SQL to Scala spark and Pyspark.\n",
      "•\tSetting up HBase column-based storage repository for archiving data on daily bases. \n",
      "•\tUsing Enterprise data lake to support various use cases including Analytics, Storing, and reporting of Voluminous, structured, and unstructured, rapidly changing data. \n",
      "•\tCreated Pipelines in ADF using Linked Services/Datasets/Pipeline/ to Extract, Transform, and load data from Different services like Azure SQL, Blob storage, Azure SQL Data warehouse, write-back tool and backwards. \n",
      "•\tMentor and support other members of the team (both on-shore and off-shore) to assist in completing tasks and meet objectives. \n",
      "Environment: Hadoop, Spark, Hive, HBase, Abinitio, Scala, Python, ETL, NoSQL (Cassandra), Azure Databricks, HDFS, MapReduce, Azure Data Lake Analytics, Spark SQL, T-SQL, U-SQL, Azure SQL, Sqoop\n",
      "\n",
      "Homesite Insurance, Boston, MA\t\t\t\t\t   Jan 2016 to Jan 2017\n",
      "Data Engineer\n",
      "•\tDesigned and implemented scalable infrastructure and platform for large amounts of data ingestion, aggregation, integration, and analytics in Hadoop, including Spark, Hive, HBase. \n",
      "•\tCreating job workflows using Oozie scheduler. \n",
      "•\tResponsible for installing, configuring, supporting, and managing of Cloudera Hadoop Clusters.\n",
      "•\tInvolved in designing, developing, testing, and documenting an application to combine personal loan, credit card and mortgage from different countries and load data to Sybase database from hive database for Reporting insights. \n",
      "•\tImplemented enterprise grade platform (Mark logic) for ETL from mainframe to NoSQL (Hbase). \n",
      "•\tTranslate business and data requirements into Logical data models in support of Enterprise Communication Data Model, OLTP, OLAP, Operational Data Store (ODS) and Analytical systems. \n",
      "•\tBuilding distributed data scalable using Hadoop. \n",
      "•\tCloudera Navigator installation and configuration using Cloudera Manager.\n",
      "•\tUsing Sqoop to load data from HDFS, Hive, MySQL, and many other sources on daily bases. \n",
      "•\tUse Python, Pyspark programming daily to perform transformations for applying business logic. \n",
      "•\tWriting Hive Queries in Spark-SQL for analysis and processing the data. \n",
      "•\tSetting up HBase column-based storage repository for archiving data on daily bases. \n",
      "•\tUsing Enterprise data lake to support various use cases including Analytics, Storing, and reporting of Voluminous, structured, and unstructured, rapidly changing data. \n",
      "•\tWorked on NiFi data Pipeline to process large set of data and configured Lookup’s for Data Validation and Integrity.\n",
      "•\tExported the analyzed data into relational databases using Sqoop for visualization and to generate reports for the BI team. \n",
      "•\tDeveloped AWS strategy, planning, and configuration of S3, Security groups, IAM, EC2, EMR and Redshift.\n",
      "•\tCreated Flatfile schemas for a particular partner data and developed services for parsing and converting the Flatfile data into EDI.\n",
      "•\tDeveloped the Pysprk code for AWS Glue jobs and for EMR and Data Extraction, aggregations, and consolidation of Adobe data within AWS Glue using PySpark. \n",
      "•\tAnalyzed the Business requirement for Oracle Data Integrator and mapped the architecture and used ODI for reverse engineering to retrieve metadata from data storage and load it to the repository.\n",
      "•\tImplemented Nifi flow topologies to perform cleansing operations before moving data into HDFS. \n",
      "•\tConverting data load pipeline algorithms written in python and SQL to Pyspark. \n",
      "•\tMentor and support other members of the team (both on-shore and off-shore) to assist in completing tasks and meet objectives.\n",
      "Environment: Hadoop, Spark, Hive, HBase, Python, ETL, NoSQL, HDFS, MapReduce, Sqoop, AWS S3, EC2, EMR, AWS Glue, NIFI, Oozie, Pyspark.\n",
      "\n",
      "\n",
      "Dhruvsoft Services Private Limited, Hyderabad, India \t\tJun 2013 to Aug 2015\n",
      "Data Analyst\n",
      "•\tInvolved in High Level Requirements gathering and Estimations, Data modeling, Data Design, Logistics, and environment set up for the project, Code reviews based on Target customized checklists. \n",
      "•\tLead offshore developers and help them assist understand the business and the necessary components for the Data Integration, Extraction, Transformation and Load. \n",
      "•\tDeveloped ETL design and ETL tool to extract data from DB2, Oracle and Xml databases. \n",
      "•\tExpertise in Build, Unit testing, System testing and User acceptance testing. \n",
      "•\tDesigned documented report processing logic, Standardized process of report interaction to non-technical business users. \n",
      "•\tDeveloped PL/SQL procedures/packages to kick off the SQL Loader control files/procedures to load the data into Oracle.\n",
      "•\tAnalyzed large data sets using pandas to identify different trends/patterns about data \n",
      "•\tParticipate in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshops/meetings with various business users.\n",
      "•\tExtract Transformation and Loading process has been implemented with the help of Informatica power center & Power exchange, Main frames, shell script which populates the database tables, used for generating the reports with Business Objects. \n",
      "•\tCoordinate Offshore team on deliverables, make sure deliverables do not impact. \n",
      "•\tBuild and maintain complex SQL queries for data analysis, data mining and data manipulation \n",
      "•\tDeveloped Matrix and tabular reports by grouping and sorting rows \n",
      "•\tActively participated in weekly meetings with the technical teams to review the code \n",
      "•\tBuilt machine learning algorithms like linear regression, decision tree, random forest for continuous variable problems, estimated machine learning algorithm's performance for time series data \n",
      "•\tUtilized regression models using SciPy to predict future data and visualized them \n",
      "•\tManaged large datasets using Panda’s data frames and MySQL for analysis purposes \n",
      "•\tDeveloped schemas to handle reporting requirements using Tableau\n",
      "•\tEnvironment: Informatica power center 9.6.1, Power exchange, Teradata database and Utilities, Visio, Oracle, Flat files, AutoSys scheduler, Unix.\n",
      "\n",
      "\n",
      "EDUCATION:\n",
      "Bachelor’s Degree in information technology from JTNU India 2013\n",
      "Master’s Degree in computers & information science Virginia 2017 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def txt_to_text(path_to_file):\n",
    "    # Open the file in read mode\n",
    "    with open(path_to_file, 'r') as file:\n",
    "        # Read the contents of the file\n",
    "        resume_text = file.read()\n",
    "    return resume_text\n",
    "\n",
    "print(txt_to_text('./assets/resume.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. text from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEET SHIHORA\n",
      "\n",
      "Surat, Gujarat, India 395006\n",
      "oJ +91 8799584983 % heetshihora04Q@gmail.com fj Heet Shihora €) HeetShihora\n",
      "\n",
      "Education\n",
      "\n",
      "Nirma University September. 2021 — Present\n",
      "B.Tech. in Computer Science Ahmedabad, Gujarat\n",
      "PPI : 8.93/10\n",
      "\n",
      "P.P. Savani Vidyabhavan June. 2019 — April 2021\n",
      "Gujarat Secondary and Higher Secondary Education Board Surat, Gujarat\n",
      "\n",
      "Percentage : 90.30%\n",
      "JEE : AIR 14747\n",
      "\n",
      "P.P. Savani Vidyabhavan June. 2018 — April 2019\n",
      "\n",
      "Gujarat Secondary Education Board Surat, Gujarat\n",
      "Percentage : 90.33%\n",
      "\n",
      "Relevant Coursework\n",
      "\n",
      "e¢ Data Structures and Algorithm e Operating System\n",
      "e Design and Analysis of Algorithm e Computer Networks\n",
      "e Machine Learning e Object Oriented Programming\n",
      "¢« Database Management System e Web development\n",
      "Projects\n",
      "Book Exchange | HTML, CSS, JavaScript At Ingenious Hackathon 4.0\n",
      "\n",
      "¢ Technology and Frameworks that our team used in this project: Bootstrap, Node.Js, JavaScript, Ejs, Cloudinary,\n",
      "Express.Js, MongoDB\n",
      "\n",
      "¢ Online platform for buying ans selling old books\n",
      "e Users can buy and sell old books with each other instead of purchasing new books from publishers\n",
      "¢ View Project | View Certificate\n",
      "\n",
      "Online Quiz Web Application | Python, Flask, BootStrap, MySql, HTML, CSS, JavaScript\n",
      "¢ Developed Python-Flask backend for a quiz app, utilizing MySQL for data storage.\n",
      "e Created database schema to store quiz ID, questions, options, and correct answers.\n",
      "e Enabled administrators to upload CSV files for quiz creation, generating unique quiz IDs.\n",
      "¢« Implemented face tracking and timer system to prevent cheating during quizzes.\n",
      "e Automatically marks users as absent if their face exceeds the frame for 10 consecutive seconds.\n",
      "e« View Project\n",
      "\n",
      "Venue Booking System | Java\n",
      "¢ Object Oriented Programming Concepts, Hashmaps, CSV file\n",
      "¢ Cli based application for Venue booking system using file handling\n",
      "e« Admin can add different venues and its available dates\n",
      "e User can book venue on available dates and changes in dates reflects in CSV file\n",
      "\n",
      "Technical and Problem Solving Skills\n",
      "\n",
      "Languages: C++, C, Python, Java, HTML, CSS, JavaScript, SQL\n",
      "\n",
      "Frameworks: React.JS, Flask, BootStrap\n",
      "\n",
      "Technologies: Git, Github\n",
      "\n",
      "Problem Solving: CodeForces: Pupil (Rating 1247), CodeChef: 3 Star (Rating 1649)\n",
      "\n",
      "Experience and Involvements\n",
      "\n",
      "Amazon ML Summer School’23 Core Committee Member\n",
      "Computer Society of India\n",
      "¢ Gaining hands-on experience in cutting-edge\n",
      "machine learning techniques and collaborating with ¢ Organized HackNuthon 4.0 and Cubix 2023 offline at\n",
      "industry experts. Nirma University Campus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def image_to_text(path_to_file):\n",
    "    # Open the image using PIL (Python Imaging Library)\n",
    "    image = Image.open(path_to_file)\n",
    "    # Perform OCR using pytesseract\n",
    "    text = pytesseract.image_to_string(image)\n",
    "\n",
    "    return text\n",
    "\n",
    "print(image_to_text('./assets/resume.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEET SHIHORA\n",
      "\n",
      "Surat, Gujarat, India 395006\n",
      "2 +91 8799584983 %heetshihora04@gmail.com ff§ Heet Shihora €) HeetShihora\n",
      "\n",
      "Education\n",
      "\n",
      "Nirma University September. 2021 — Present\n",
      "B.Tech. in Computer Science Ahmedabad, Gujarat\n",
      "PPI : 8.93/10\n",
      "\n",
      "P.P. Savani Vidyabhavan June. 2019 — April 2021\n",
      "Gujarat Secondary and Higher Secondary Education Board Surat, Gujarat\n",
      "\n",
      "Percentage : 90.30%\n",
      "JEE : AIR 14747\n",
      "\n",
      "P.P. Savani Vidyabhavan June. 2018 — April 2019\n",
      "\n",
      "Gujarat Secondary Education Board Surat, Gujarat\n",
      "Percentage : 90.33%\n",
      "\n",
      "Relevant Coursework\n",
      "\n",
      "¢ Data Structures and Algorithm ¢ Operating System\n",
      "¢ Design and Analysis of Algorithm ¢ Computer Networks\n",
      "¢ Machine Learning ¢ Object Oriented Programming\n",
      "¢ Database Management System e Web development\n",
      "Projects\n",
      "Book Exchange | HTML, CSS, JavaScript At Ingenious Hackathon 4.0\n",
      "\n",
      "¢ Technology and Frameworks that our team used in this project: Bootstrap, Node.Js, JavaScript, Ejs, Cloudinary,\n",
      "Express.Js, MongoDB\n",
      "\n",
      "e Online platform for buying ans selling old books\n",
      "\n",
      "e Users can buy and sell old books with each other instead of purchasing new books from publishers\n",
      "\n",
      "¢ View Project | View Certificate\n",
      "\n",
      "Online Quiz Web Application | Python, Flask, BootStrap, MySql, HTML, CSS, JavaScript\n",
      "¢ Developed Python-Flask backend for a quiz app, utilizing MySQL for data storage.\n",
      "¢ Created database schema to store quiz ID, questions, options, and correct answers.\n",
      "¢ Enabled administrators to upload CSV files for quiz creation, generating unique quiz IDs.\n",
      "¢ Implemented face tracking and timer system to prevent cheating during quizzes.\n",
      "¢ Automatically marks users as absent if their face exceeds the frame for 10 consecutive seconds.\n",
      "e View Project\n",
      "\n",
      "Venue Booking System | Java\n",
      "¢ Object Oriented Programming Concepts, Hashmaps, CSV file\n",
      "e Cli based application for Venue booking system using file handling\n",
      "e« Admin can add different venues and its available dates\n",
      "¢ User can book venue on available dates and changes in dates reflects in CSV file\n",
      "\n",
      "Technical and Problem Solving Skills\n",
      "\n",
      "Languages: C++, C, Python, Java, HTML, CSS, JavaScript, SQL\n",
      "\n",
      "Frameworks: React.JS, Flask, BootStrap\n",
      "\n",
      "Technologies: Git, Github\n",
      "\n",
      "Problem Solving: CodeForces: Pupil (Rating 1247), CodeChef: 3 Star (Rating 1649)\n",
      "\n",
      "Experience and Involvements\n",
      "\n",
      "Amazon ML Summer School’23 Core Committee Member\n",
      "Computer Society of India\n",
      "¢ Gaining hands-on experience in cutting-edge\n",
      "machine learning techniques and collaborating with ¢ Organized HackNuthon 4.0 and Cubix 2023 offline at\n",
      "industry experts. Nirma University Campus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pdf_to_text(path_to_file):\n",
    "    # Convert each page of the PDF to images\n",
    "    pages = convert_from_path(path_to_file, dpi=300, poppler_path=r\"./tools/poppler-24.02.0/Library/bin/\")\n",
    "\n",
    "    text_data = ''\n",
    "    for page in pages:\n",
    "        text = pytesseract.image_to_string(page)\n",
    "        text_data += text + '\\n'\n",
    "        \n",
    "    return text_data\n",
    "\n",
    "print(pdf_to_text('./assets/Heet_Shihora.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. text from docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Srikanth\n",
      "+1(415)-787-2274\n",
      "Srikanthk0718@gmail.com \n",
      "Senior Data Engineer\n",
      "\n",
      "\n",
      "SUMMARY:\n",
      "A Senior Data Engineer with over 9+ years’ experience in Information Technology in all phases of Software Development with Agile methodology which includes User Interaction, Business Analysis/Modeling, Design & Development, Integration, Planning and testing, migration and documentation in applications using ETL pipelines and distributed applications. \n",
      "Has expertise in data engineering for over 4 years using Hadoop big data technologies Apache Spark, Scala, python, Kafka, Cassandra, Jenkins Pipelines, Kubernetes, Kibana, Rancher, GITHUB, Rancher, Kibana, Hadoop HDFS, Hive, IntelliJ, Cassandra, SQL server etc. \n",
      "Worked with various transformations like Normalizer, expression, rank, filter, group, aggregator, lookups, joiner, sequence generator, sorter, SQLT, stored procedure, Update strategy, Source Qualifier, Transaction Control, Union, CDC etc., \n",
      "Designing and developing Spark jobs with data frames using different file formats like Text, Sequence, Xml, parquet, and Avro.\n",
      "Experience working with NoSQL database technologies, including MongoDB, Cassandra, and HBase. \n",
      "Excellent experience with Requests, NumPy, Matplotlib, SciPy, PySpark and Pandas python libraries during development lifecycle and experience in developing APIs for the application using Python. \n",
      "Constructing and manipulating large datasets of structured, semi-structured, and unstructured data and supporting system application architecture using tools like SAS, SQL, Python, R. \n",
      "Proficient of AWS services like VPC, Glue Pipelines, Glue Crawler, Cloud front, EC2, ECS, EKS, Elastic bean stalk, Lambda, S3, Storage gateway, RDBS, Dynamo db, Redshift, Elastic Cache, DMS, SMS, Data Pipeline, IAM, WAF, Artifacts, API gateway, SNS, SQS, SES, Auto Scaling, Cloud Formation, Cloud Watch and Cloud Trail. \n",
      "Good Knowledge on Big Data Tools such as Apache Kafka, Apache Spark, Airflow, Hive, Sqoop, Delta Lake with different file formats like Avro, Csv and Parquet. \n",
      "Expertise building data lakes and data warehouses in AWS using a variety of cloud services i.e., S3, RDS, EC2) and Azure services (Azure Data Lake, Azure Blob Storage, Azure Synapse Analytics, ADF) and processing the data in Azure Databricks. \n",
      "Strong experience with spark real time streaming data using Kafka. \n",
      "Implemented large Lambda architectures using Azure Data platform capabilities like Azure Data Lake, Azure Data \n",
      "Factory, HDInsight, Azure SQL Server, Azure ML and Power BI. \n",
      "Validating data against files and performing technical data quality checks to certify source and target/business usage. \n",
      "Very good in data modeling knowledge in Dimensional Data modeling, Star Schema, Snow-Flake Schema, FACT and Dimensions tables. \n",
      "Worked on optimizing volumes and EC2 instances and created multiple VPC instances. Deployed applications on AWS using Elastic Beanstalk and Implemented and set up Route53 for AWS Web Instances. \n",
      "Coordinating with Business Users, functional Design team and testing team during the different phases of project development and resolving the issues. \n",
      "Worked with different database oracle, SQL Server, Teradata, Cassandra SQL Programming. \n",
      "Good knowledge on Teradata and Snowflake databases.\n",
      "\n",
      "\n",
      "TECHNICAL SKILLS:\n",
      "\n",
      "\n",
      "Big Data Tools: Hadoop Ecosystem: Map Reduce, Spark 2.3, Airflow 1.10.8, Nifi 2, HBase 1.2, Hive 2.3, Pig 0.17 Sqoop 1.4, Kafka 1.0.1, Oozie 4.3, Hadoop 3.0\n",
      "BI Tools: SSIS, SSRS, SSAS.\n",
      "Data Modeling Tools: Erwin Data Modeler, ER Studio v17\n",
      "Programming Languages: SQL, PL/SQL, and UNIX.\n",
      "Methodologies: RAD, JAD, System Development Life Cycle (SDLC), Agile\n",
      "Cloud Platform: AWS, Azure, Google Cloud.\n",
      "Cloud Management: Amazon Web Services (AWS)- EC2, EMR, S3, Redshift, EMR, Lambda, Athena\n",
      "Databases: Oracle 12c/11g, Teradata R15/R14.\n",
      "OLAP Tools: Tableau, SSAS, Business Objects, and Crystal Reports 9\n",
      "ETL/Data warehouse Tools: Informatica 9.6/9.1, and Tableau.\n",
      "Operating System: Windows, Unix, Sun Solaris\n",
      "\n",
      "\n",
      "PROFESSIONAL EXPERIENCE:\n",
      "John Deere - Urbandale - Iowa\t\t\t\t        May 2020 to Present\n",
      "Senior Data Engineer\n",
      "Participate in requirement grooming meetings which involves understanding functional requirements from business perspective and providing estimates to convert those requirements into software solutions (Design and Develop & Deliver the Code to IT/UAT/PROD and validate and manage data Pipelines from multiple applications with fast-paced Agile Development methodology using Sprints with JIRA Management Tool) \n",
      "Creation and deployment of Spark jobs in different environments and loading data to no SQL database Cassandra/Hive/HDFS. Secure the data by implementing encryption-based \n",
      "Implemented AWS solutions using E2C, S3, RDS, EBS, Elastic Load Balancer, Glue Pipelines, Glue Crawler, Auto scaling groups, Optimized volumes, and EC2 instances and created monitors, alarms, and notifications for EC2 hosts using Cloud Watch. \n",
      "Experience in developing multiple MapReduce programs in java for data extraction, transformation and aggregation from multiple file formats including XML, JSON, CSV, and other file formats. \n",
      "Developing code using: Apache Spark and Scala, IntelliJ, NoSQL databases (Cassandra), Jenkins, Docker pipelines, GITHUB, Kubernetes, HDFS file System, Hive, Kafka for streaming Real time streaming data, Kibana for monitor logs. \n",
      "Analysis on existing data flows and create high level/low level technical design documents for business stakeholders that confirm technical design aligns with business requirements. \n",
      "Extensively worked on Data Modeling involving Dimensional Data modeling, Star Schema/Snowflake schema, FACT & Dimensions tables, Physical & logical data modeling. \n",
      "Developed data models and data migration strategies utilizing concepts of snowflake schema. \n",
      "Involving in testing the database using complex SQL scripts and handling the performance issues effectively. \n",
      "Handled importing of data from various data sources, performed transformations using Hive, MapReduce, loaded data into HDFS and extracted data from MYSQL into HDFS vice-versa using Sqoop. \n",
      "ETL development using EMR/Hive/Spark, Lambda, Scala, DynamoDB Streams, Amazon Kinesis Firehose, Redshift and S3. \n",
      "Loaded the data into Spark RDD, perform advanced procedures like text analytics and processing using in-memory data computation’s capabilities of Spark using Scala to generate the Output response. \n",
      "Used Spark API over Cloudera Hadoop Yarn to perform analytics on data in Hive. \n",
      "Developed Scala scripts using both Data frames/SQL and RDD/MapReduce in Spark for Data Aggregation, queries and writing data back into the OLTP system through Sqoop.\n",
      "Wrote various data normalization jobs for new data ingested into Redshift. \n",
      "Used AWS S3 Buckets to store the file and injected the files into Snowflake tables using Snow Pipe and run deltas using Data pipelines. \n",
      "Handled large datasets using Partitions, Spark in Memory capabilities, Broadcasts in Spark, Effective efficient Joins, Transformations, and others during the ingestion process itself. \n",
      "Worked with AWS cloud platform and its features which include EC2, IAM, EBS CloudWatch and AWS S3 Deployed application using AWS EC2 standard deployment techniques and worked on AWS infrastructure and automation. \n",
      "Installed and configured Apache airflow for workflow management and created workflows in python. \n",
      "Developed python code for different tasks, dependencies, SLA watcher and time sensor for each job for workflow management and automation using Airflow tool. \n",
      "Implementing the strategy to migrate Netezza based analytical systems to Snowflake on AWS. \n",
      "Environment: Apache spark 2.4.5, Scala2.1.1, Cassandra, HDFS, Hive, GitHub, Jenkins, Kafka, Informatica PowerCenter 10.x, SQL Server 2008, Salesforce Cloud, Visio, TOAD, Putty, Autosys Scheduler, UNIX, AWS, snowflake, CSV, WinSCP, python\n",
      "\n",
      "American Airlines, Fort Worth, Texas\t\t\t\t               Feb 2017 to Apr 2020\n",
      "Senior Data Engineer\n",
      "Designed and implemented scalable infrastructure and platform for large amounts of data ingestion, aggregation, integration, and analytics in Hadoop, including Spark, Hive, HBase. \n",
      "Creating job workflows using Oozie scheduler. \n",
      "Developing an architecture to move the project from Abinitio to Pyspark and Scala spark. \n",
      "Implemented enterprise grade platform (Mark logic) for ETL from mainframe to NoSQL (Cassandra). \n",
      "Extract Transform and Load data from Sources Systems to Azure Data Storage services using a combination of Azure Data Factory, T-SQL, Spark SQL, and U-SQL Azure Data Lake Analytics. Data Ingestion to one or more Azure Services - (Azure Data Lake, Azure Storage, Azure SQL, Azure DW) and processing the data in In Azure Databricks. \n",
      "Use Python, Scala programming daily to perform transformations for applying business logic. \n",
      "Involved in designing, developing, testing, and documenting an application to combine personal loan, credit card and mortgage from different countries and load data to Sybase database from hive database for Reporting insights.\n",
      "Building distributed data scalable using Hadoop. \n",
      "Using Sqoop to load data from HDFS, Hive, MySQL, and many other sources on daily bases. \n",
      "Creating MapReduce programs to enable data for transformation, extraction, and aggregation of multiple formats like Avro, Parquet, XML, JSON, CSV, and other compressed file formats. \n",
      "Writing Hive Queries in Spark-SQL for analysis and processing the data.\n",
      "Exported the analyzed data into relational databases using Sqoop for visualization and to generate reports for the BI team. \n",
      "Converting data load pipeline algorithms written in python and SQL to Scala spark and Pyspark.\n",
      "Setting up HBase column-based storage repository for archiving data on daily bases. \n",
      "Using Enterprise data lake to support various use cases including Analytics, Storing, and reporting of Voluminous, structured, and unstructured, rapidly changing data. \n",
      "Created Pipelines in ADF using Linked Services/Datasets/Pipeline/ to Extract, Transform, and load data from Different services like Azure SQL, Blob storage, Azure SQL Data warehouse, write-back tool and backwards. \n",
      "Mentor and support other members of the team (both on-shore and off-shore) to assist in completing tasks and meet objectives. \n",
      "Environment: Hadoop, Spark, Hive, HBase, Abinitio, Scala, Python, ETL, NoSQL (Cassandra), Azure Databricks, HDFS, MapReduce, Azure Data Lake Analytics, Spark SQL, T-SQL, U-SQL, Azure SQL, Sqoop\n",
      "\n",
      "Homesite Insurance, Boston, MA\t\t\t\t\t   Jan 2016 to Jan 2017\n",
      "Data Engineer\n",
      "Designed and implemented scalable infrastructure and platform for large amounts of data ingestion, aggregation, integration, and analytics in Hadoop, including Spark, Hive, HBase. \n",
      "Creating job workflows using Oozie scheduler. \n",
      "Responsible for installing, configuring, supporting, and managing of Cloudera Hadoop Clusters.\n",
      "Involved in designing, developing, testing, and documenting an application to combine personal loan, credit card and mortgage from different countries and load data to Sybase database from hive database for Reporting insights. \n",
      "Implemented enterprise grade platform (Mark logic) for ETL from mainframe to NoSQL (Hbase). \n",
      "Translate business and data requirements into Logical data models in support of Enterprise Communication Data Model, OLTP, OLAP, Operational Data Store (ODS) and Analytical systems. \n",
      "Building distributed data scalable using Hadoop. \n",
      "Cloudera Navigator installation and configuration using Cloudera Manager.\n",
      "Using Sqoop to load data from HDFS, Hive, MySQL, and many other sources on daily bases. \n",
      "Use Python, Pyspark programming daily to perform transformations for applying business logic. \n",
      "Writing Hive Queries in Spark-SQL for analysis and processing the data. \n",
      "Setting up HBase column-based storage repository for archiving data on daily bases. \n",
      "Using Enterprise data lake to support various use cases including Analytics, Storing, and reporting of Voluminous, structured, and unstructured, rapidly changing data. \n",
      "Worked on NiFi data Pipeline to process large set of data and configured Lookup’s for Data Validation and Integrity.\n",
      "Exported the analyzed data into relational databases using Sqoop for visualization and to generate reports for the BI team. \n",
      "Developed AWS strategy, planning, and configuration of S3, Security groups, IAM, EC2, EMR and Redshift.\n",
      "Created Flatfile schemas for a particular partner data and developed services for parsing and converting the Flatfile data into EDI.\n",
      "Developed the Pysprk code for AWS Glue jobs and for EMR and Data Extraction, aggregations, and consolidation of Adobe data within AWS Glue using PySpark. \n",
      "Analyzed the Business requirement for Oracle Data Integrator and mapped the architecture and used ODI for reverse engineering to retrieve metadata from data storage and load it to the repository.\n",
      "Implemented Nifi flow topologies to perform cleansing operations before moving data into HDFS. \n",
      "Converting data load pipeline algorithms written in python and SQL to Pyspark. \n",
      "Mentor and support other members of the team (both on-shore and off-shore) to assist in completing tasks and meet objectives.\n",
      "Environment: Hadoop, Spark, Hive, HBase, Python, ETL, NoSQL, HDFS, MapReduce, Sqoop, AWS S3, EC2, EMR, AWS Glue, NIFI, Oozie, Pyspark.\n",
      "\n",
      "\n",
      "Dhruvsoft Services Private Limited, Hyderabad, India \t\tJun 2013 to Aug 2015\n",
      "Data Analyst\n",
      "Involved in High Level Requirements gathering and Estimations, Data modeling, Data Design, Logistics, and environment set up for the project, Code reviews based on Target customized checklists. \n",
      "Lead offshore developers and help them assist understand the business and the necessary components for the Data Integration, Extraction, Transformation and Load. \n",
      "Developed ETL design and ETL tool to extract data from DB2, Oracle and Xml databases. \n",
      "Expertise in Build, Unit testing, System testing and User acceptance testing. \n",
      "Designed documented report processing logic, Standardized process of report interaction to non-technical business users. \n",
      "Developed PL/SQL procedures/packages to kick off the SQL Loader control files/procedures to load the data into Oracle.\n",
      "Analyzed large data sets using pandas to identify different trends/patterns about data \n",
      "Participate in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshops/meetings with various business users.\n",
      "Extract Transformation and Loading process has been implemented with the help of Informatica power center & Power exchange, Main frames, shell script which populates the database tables, used for generating the reports with Business Objects. \n",
      "Coordinate Offshore team on deliverables, make sure deliverables do not impact. \n",
      "Build and maintain complex SQL queries for data analysis, data mining and data manipulation \n",
      "Developed Matrix and tabular reports by grouping and sorting rows \n",
      "Actively participated in weekly meetings with the technical teams to review the code \n",
      "Built machine learning algorithms like linear regression, decision tree, random forest for continuous variable problems, estimated machine learning algorithm's performance for time series data \n",
      "Utilized regression models using SciPy to predict future data and visualized them \n",
      "Managed large datasets using Panda’s data frames and MySQL for analysis purposes \n",
      "Developed schemas to handle reporting requirements using Tableau\n",
      "Environment: Informatica power center 9.6.1, Power exchange, Teradata database and Utilities, Visio, Oracle, Flat files, AutoSys scheduler, Unix.\n",
      "\n",
      "\n",
      "EDUCATION:\n",
      "Bachelor’s Degree in information technology from JTNU India 2013\n",
      "Master’s Degree in computers & information science Virginia 2017 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def docx_to_text(path_to_file):\n",
    "    doc = Document(path_to_file)\n",
    "    text_content = ''\n",
    "\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text_content += paragraph.text + '\\n'\n",
    "\n",
    "    return text_content\n",
    "\n",
    "print(docx_to_text('./assets/resume.docx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. text from html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thomas Leon Highbaugh\n",
      "\n",
      "Full Stack Web Development\n",
      "\n",
      "San Francisco, California\n",
      "\n",
      "T\n",
      "\n",
      "L\n",
      "\n",
      "H\n",
      "\n",
      "Portfolio:                                              thomasleonhighbaugh.me                                                ↗\n",
      "\n",
      "Github:                                              Thomashighbaugh                                                ↗\n",
      "\n",
      "Email:                        me@thomasleonhighbaugh.me                                                ↗\n",
      "\n",
      "Phone:                                              +1(510)907-0654\n",
      "\n",
      "SUMMARY\n",
      "\n",
      "Experienced full-stack web developer with a strong track record of independently addressing complex business requirements and overcoming challenges to deliver polished and user-friendly web solutions.\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "California State University East Bay\n",
      "\n",
      "2009 – 2014 | Bachelor of Arts\n",
      "\n",
      "Major:                                          Political Science\n",
      "\n",
      "Minor:                                          Economics\n",
      "\n",
      "GPA:                                          3.9\n",
      "\n",
      "Skills:                                          Research, Analysis, Critical Thinking, Communication\n",
      "\n",
      "Las Positas Community College\n",
      "\n",
      "2018 – Present | Associate of Science/Continuing\n",
      "                    Education\n",
      "\n",
      "Major:                                          Computer Science\n",
      "\n",
      "GPA:                                          4.0\n",
      "\n",
      "Skills:                                          Programming, Algorithms, Data Structures, Software Development\n",
      "\n",
      "Codify Academy\n",
      "\n",
      "2018 | Certificate\n",
      "\n",
      "Subject:                                          Front End Development\n",
      "\n",
      "Skills:                                          HTML, CSS, JavaScript, Web Development\n",
      "\n",
      "SKILLS\n",
      "\n",
      "HTML5\n",
      "\n",
      "CSS3\n",
      "\n",
      "JavaScript\n",
      "\n",
      "TypeScript\n",
      "\n",
      "Node.js\n",
      "\n",
      "React.js\n",
      "\n",
      "Python\n",
      "\n",
      "NoSQL\n",
      "\n",
      "Postgresql\n",
      "\n",
      "MongoDB\n",
      "\n",
      "Linux\n",
      "\n",
      "AWS\n",
      "\n",
      "LLM/AI Prompting\n",
      "\n",
      "Web Design\n",
      "\n",
      "UI/UX\n",
      "\n",
      "CI/CD\n",
      "\n",
      "Lua\n",
      "\n",
      "EXPERIENCE\n",
      "\n",
      "Full Stack Web Developer\n",
      "\n",
      "Jun 2018 – Present | Freelance\n",
      "\n",
      "›                                          Created high-quality, customized web applications from scratch, employing a diverse set of programming languages, including HTML, CSS, JavaScript, PHP, and Python, to fulfill unique client requirements.\n",
      "\n",
      "›                                          Leveraged a range of bleeding edge front-end frameworks like React.js, Nullstack and Vue.js, as well as back-end frameworks such as Node.js and Express, to deliver robust and scalable web solutions.\n",
      "\n",
      "›                                          Designed and implemented databases using SQL and NoSQL technologies like MySQL, PostgreSQL, MongoDB, and Firebase, optimizing data storage and management.\n",
      "\n",
      "Computer Repair Technician\n",
      "\n",
      "Mar 2018 – Present | Freelance\n",
      "\n",
      "›                                          Provided expert computer repair services to individuals and businesses, diagnosing and resolving hardware and software issues effectively.\n",
      "\n",
      "›                                          Demonstrated proficiency across diverse operating systems (Windows, macOS, Linux) and hardware components, offering tailored solutions, including hardware upgrades, software installations, and malware removal.\n",
      "\n",
      "Assistant Manager\n",
      "\n",
      "May 2012 – Apr 2018 | Pet Food Express\n",
      "\n",
      "›                                          Trained and supervised staff in product knowledge, customer engagement, and sales techniques, ensuring a top-notch shopping experience for customers.\n",
      "\n",
      "›                                          Maintained a pristine and organized store environment, fostering maximum customer satisfaction.\n",
      "\n",
      "›                                          Demonstrated deep expertise in the pet food industry, including trends, products, and competitors, providing customers with informed recommendations and advice.\n",
      "\n",
      "Executive Assistant\n",
      "\n",
      "Jun 2009 – Dec 2011 | 360 Custom Closets and Cabinets\n",
      "\n",
      "›                                          Provided high-level administrative support to the small business executive, managing schedules, travel arrangements, and coordinating meetings and events.\n",
      "\n",
      "›                                          Managed communication on behalf of the executive, including answering phone calls and emails, ensuring timely and professional responses to inquiries and requests.\n",
      "\n",
      "›                                          Maintained relationships with vendors, clients, and external stakeholders, ensuring professionalism and alignment with the business's values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def html_to_text(path_to_file):\n",
    "    text_content = textract.process(path_to_file).decode('utf-8')\n",
    "    return text_content\n",
    "\n",
    "print(html_to_text('./assets/resume.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get text from given file type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get file type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_file_type(file_path):\n",
    "#     file_magic = magic.Magic(mime=True)\n",
    "#     mime_type = file_magic.from_file(file_path)\n",
    "    \n",
    "#     # Or:\n",
    "    \n",
    "#     # mime_type = magic.from_file(file_path)\n",
    "    \n",
    "#     #  Or:\n",
    "    \n",
    "#     # Open the file and read its contents\n",
    "#     # with open(file_path, 'rb') as f:\n",
    "#         # Get the MIME type of the file\n",
    "#         # mime_type = magic.from_buffer(f.read(1024))\n",
    "        \n",
    "#     return mime_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. get text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_file(path_to_file):\n",
    "    if not os.path.exists(path_to_file):\n",
    "        return \"File not found\"\n",
    "\n",
    "    file_name, file_extension = os.path.splitext(path_to_file)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == \".txt\":\n",
    "        return txt_to_text(path_to_file)\n",
    "    \n",
    "    elif file_extension in (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".tiff\"):\n",
    "        return image_to_text(path_to_file)\n",
    "    \n",
    "    elif file_extension == \".pdf\":\n",
    "        return pdf_to_text(path_to_file)\n",
    "    elif file_extension == \".docx\":\n",
    "        return docx_to_text(path_to_file)\n",
    "    elif file_extension in (\".html\", \".htm\"):\n",
    "        return html_to_text(path_to_file)\n",
    "    else:\n",
    "        return \"Unknown file type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Last Updated on 14th February 2024\\n\\nyaenik Vasoya\\n\\nyagnikvasoya1125@gmail.com | +91 8866337492\\n\\nEDUCATION\\n\\nNIRMA UNIVERSITY\\n\\nB.TECH IN COMPUTER SCIENCE AND ENGINEERING\\nOctober 2021 - Present | Anmedabad\\n\\nCGPA: 8.7/10\\n\\nTHE SCHOOL OF SCIENCE\\nJune 2020 - June 2021 | Rajkot\\n12th Board (GSEB) : 97.69%\\n\\nGENIUS PUBLIC SCHOOL\\nJune 2012 - March 2019 | Rajkot\\n10th Board (GSEB) : 95.00%\\n\\nRECENT PROJECTS\\nWHATSAPP CLONE | FLUTTER\\n\\nSER 2Q72ated a WhatsApp Clone using Flutter language technologies.\\ne The project uses Firebase cloud database to store the details of the user.\\ne | used firebase database to store chat of user as well.\\nSite\\n\\nCODEFORCES API APP | FLUTTER\\nJan 2024\\n\\ne | created an app using Flutter language technologies with Codeforces Apis.\\n\\ne The project Fetches data in Json format from the Api and manipulate it.\\ne | used Flutter framework to show fetched data in readable format.\\nSite\\n\\nSECURE FILE MANAGER | PYTHON, ORACLE DATABASE, ML\\nApr 2023\\ne |tis a GUI based application. User can make their accounts.\\nUser will be authenticate using password or using face detection.\\ne Secure storage of files in an Oracle database.\\ne Encryption of files to protect them from unauthorized access.\\ne User-friendly interface for easy fille management.\\ne Multiple Users can store and access files form a single device.\\nGitHub\\n\\nANIME WEBSITE | HTML, CSS, JS\\nApr 2023\\ne | created an Anime Website using HTML, CSS and JS.\\ne | created this website/blog as an introduction for new Anime fans with\\nattractive designs and information.\\ne | used CSS for style and animation and JS to get the visitors review for my\\nwebsite\\nSite\\n\\nLINKS\\n\\nLINKEDIN: Yagnik Vasoya\\nGITHuB: yagnik1125\\nCODEFORCEs: Yagnik_Vasoya\\nCODECHEF: yagnikvasoya25\\n\\nSKILLS\\n\\nPROGRAMMING LANGUAGES\\ne c/c++ | PROBLEM SOLVING\\n\\ne Python | ML,DL\\ne JAVA\\ne SQL\\nWEB TECHNOLOGIES\\ne HTML| CSS | JavaScript\\n\\ne Flutter | SOFTWARE\\nDEVELOPMENT\\n\\ne Firebase\\n\\nACHIEVEMENTS\\n\\ne Specialist at Codeforces (Max\\nRating: 1449)\\n\\ne Two Stars at CodeChef (Max\\nRating: 1492)\\n\\nCOURSEWORK\\n\\ne Data Structures and Algo-\\nrithms\\n\\n¢ Design And Analysis Of Algo-\\nrithms\\n\\ne Object Oriented Program-\\nming\\n\\ne Operating System\\n¢ Computer Networks\\n\\ne Database Management Sys-\\ntem\\n\\ne Web Technologies, Flutter\\n\\nINTERESTS\\n\\ne Explore the tech world\\ne Playing Cricket\\n\\ne Music\\n\\n\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = get_text_from_file('./assets/Shrut_Vanpariya_latest_52.pdf')\n",
    "text = get_text_from_file('./assets/Yagnik_Resume.pdf')\n",
    "# text = get_text_from_file('./assets/resume.doc')\n",
    "# text = get_text_from_file('./assets/resume.docx')\n",
    "# text = get_text_from_file('./assets/resume.txt')\n",
    "# text = get_text_from_file('./assets/resume.jpg')\n",
    "# text = get_text_from_file('./assets/resume.html')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from this resume files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shrut\\OneDrive\\Desktop\\Mined2024\\env\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.2.4 and may not be 100% compatible with the current version (3.7.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\shrut\\OneDrive\\Desktop\\Mined2024\\env\\Lib\\site-packages\\spacy_transformers\\layers\\hf_shim.py:137: UserWarning: Error loading saved torch state_dict with strict=True, likely due to differences between 'transformers' versions. Attempting to load with strict=False as a fallback...\n",
      "\n",
      "If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current 'transformers' and 'spacy-transformers' versions. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load(\"./model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIRMA UNIVERSITY ->> ORG\n",
      "CSS ->> TOOL\n",
      "JS ->> TOOL\n",
      "Yagnik Vasoya ->> TOOL\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(text)\n",
    "# print(doc.ents)\n",
    "for ent in doc.ents:\n",
    "  print(ent.text,\"->>\", ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mined2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
